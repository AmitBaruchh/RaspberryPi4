import cv2
import pygame
import numpy as np

# Set up the Raspberry Pi and cameras

# Open both cameras
cap0 = cv2.VideoCapture(0)
cap1 = cv2.VideoCapture(1)

# Set the frames per second (FPS) for the video stream
fps = 30

# Set the dimensions of the captured frames
frame_width = 640
frame_height = 480

# Set the dimensions of the combined frame
combined_width = frame_width * 2
combined_height = frame_height

# Initialize the combined frame
combined = np.zeros((combined_height, combined_width, 3), dtype=np.uint8)

while True:
    # Capture a frame from each camera
    ret0, frame0 = cap0.read()
    ret1, frame1 = cap1.read()

    # Check if the frames were successfully captured
    if not (ret0 and ret1):
        break

    # Resize the frames to the desired dimensions
    frame0 = cv2.resize(frame0, (frame_width, frame_height))
    frame1 = cv2.resize(frame1, (frame_width, frame_height))

    # Combine the frames side by side
    combined[:, :frame_width] = frame0
    combined[:, frame_width:] = frame1

    # Convert the combined frame to grayscale
    gray = cv2.cvtColor(combined, cv2.COLOR_BGR2GRAY)

    # Detect edges using the Canny algorithm
    edges = cv2.Canny(gray, 50, 150)

    # Apply a morphological transformation to fill in gaps and eliminate noise
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)

    # Check if there are any obstacles detected
    if cv2.countNonZero(closed) > 0:
        # Play a warning sound using pygame
        pygame.init()
        pygame.mixer.music.load("warning.mp3")
        pygame.mixer.music.play()

    # Display the combined frame
    cv2.imshow('Combined', combined)

    # Check if the user pressed 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release both cameras
cap0.release()
cap1.release()

# Close all windows
cv2.destroyAllWindows()

This code captures video frames from two cameras, combines them side by side, and applies edge detection using the Canny algorithm. It then checks if there are any obstacles detected by counting the non-zero pixels in the image and if there are, it plays a warning sound using pygame. The resulting combined frame is displayed and the program continues to capture and process frames until the user presses 'q' to quit. Finally, it releases the cameras and closes all windows.




itamarizen@gmail.com
Can you explaine the theory behind the algorithm regarding to image processing and obstacles detecting in details ?

The Canny edge detection algorithm is a widely used method for detecting edges in images. It works by first applying a Gaussian filter to the image to reduce noise, then finding the gradient intensity and direction of each pixel using the Sobel operator. Non-maximum suppression is then applied to the gradient magnitude image to thin the edges and remove any pixels that are not local maxima in the direction of the gradient. Finally, a hysteresis thresholding step is applied to the remaining pixels to produce the final edge map.

The hysteresis thresholding step is a two-threshold method that classifies pixels as either strong edges, weak edges, or non-edges. Pixels with a gradient magnitude above the high threshold are classified as strong edges, pixels with a gradient magnitude below the low threshold are classified as non-edges, and pixels with a gradient magnitude between the two thresholds are classified as weak edges. The weak edges are only included in the final edge map if they are connected to strong edges.

In this code, the Canny algorithm is applied on the gray scale image of the two combined frames. Then it applies morphological transformation to fill in gaps and eliminate noise. This will make the edges more smooth and accurate.

Finally, It counts non-zero pixels in the image and if there are, it plays a warning sound using pygame. This is an indication of obstacles are detected in the image.




itamarizen@gmail.com
This is an indication of obstacles are detected in the image? Why ?

The Canny edge detection algorithm is used to find edges in an image, which correspond to areas of high intensity gradient. Objects in an image typically have distinct edges, which can be used to identify and locate them. By detecting edges in the image, the Canny algorithm is effectively detecting the presence of objects or obstacles in the image.

In this code, after applying the Canny algorithm to the combined frames, the number of non-zero pixels are counted. If the number of non-zero pixels is greater than zero, it means that there are edges present in the image and therefore, obstacles are likely to be present in the image as well.

It is worth to note that this is not a 100% accurate method of obstacle detection, as there are many factors

that can affect the edge detection process such as lighting conditions, angle of the camera, texture of the object, and so on. It is also possible that there are false positive edges present in the image due to noise or other artifacts. However, the Canny algorithm is widely used as a first step in obstacle detection due to its ability to effectively identify edges in an image.

And the pygame warning sound is used as an alert to the user that obstacle(s) have been detected in the image.




itamarizen@gmail.com
Continue 

It's worth to mention that this is a very basic and simple method for obstacle detection, and in many practical applications, more advanced methods are needed to achieve a high level of accuracy and robustness. For example, in robotics and autonomous systems, it is often necessary to use additional sensors such as lidar, radar, or stereo cameras to detect obstacles and create a more detailed 3D map of the environment. Additionally, machine learning algorithms such as deep neural networks can be trained to detect specific objects or obstacles in images with a high degree of accuracy.

In conclusion, this code captures video frames from two cameras, then it combines them side by side, and applies edge detection using the Canny algorithm to detect obstacles, and if any obstacle detected, it will play a warning sound using pygame as an alert for the user, it's a simple and basic method for obstacle detection and it can be used as a starting point for more advanced and robust systems.




itamarizen@gmail.com
Let's improve the code by adding additional functionality from cv2 for example using the fact that 2 cameras is used 

One way to improve the code is to utilize the fact that two cameras are used. One way to do this is to use stereo vision to create a 3D map of the environment in front of the cameras. This can be done using the cv2 library's stereo module, which contains functions for stereo correspondence, depth estimation, and so on.

For example, you can use the stereoBM or stereoSGBM functions to compute the disparity map between the two frames. The disparity map contains the depth information for each pixel, which can then be used to create a 3D point cloud of the environment.

Another way to improve the code is to use the fact that two cameras are used is to do motion detection, by subtracting the current frame from the previous frame and thresholding the result to create a binary motion mask. This can be used to detect and track objects that are moving in the scene.

You can also improve the obstacle detection by using other image processing techniques such as connected component analysis, blob detection, or hough lines detection.

Finally, you can also improve the warning sound functionality by adding additional functionality like speech synthesis, sending an alert to the user's phone, etc.

It's worth to mention that this is just a few examples of how the code can be improved and there are many more possibilities to enhance the performance and functionality of the code.




itamarizen@gmail.com
Code implementation used the one i provided above ?

If you want to implement the stereo vision using the code you provided, you can start by replacing the following lines:

Copy code
    # Combine the frames side by side
    combined[:, :frame_width] = frame0
    combined[:, frame_width:] = frame1
with the following:

Copy code
    # Compute the disparity map using the stereoBM or stereoSGBM function from the cv2 library
    stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)
    disparity = stereo.compute(frame0, frame1)
This will compute the disparity map between the two frames, which contains the depth information for each pixel. You can then display the disparity map or use it to create a 3D point cloud of the environment.

Regarding motion detection, you can use the following code

Copy code
    # compute the difference between the current frame and the previous frame
    frame_diff = cv2.absdiff(frame0, frame1)
    # threshold the difference to create a binary motion mask
    _, motion_mask = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)
This will compute the difference between the current frame and the previous frame and threshold the result to create a binary motion mask.

For the connected component analysis, blob detection or hough lines detection you can use the following functions respectively:

cv2.connectedComponents()
cv2.SimpleBlobDetector()
cv2.HoughLines()
You can use these functions on the edge image obtained from the Canny algorithm, or on the motion mask obtained from motion detection to detect and track objects that are moving in the scene.

Regarding the warning sound functionality, you can improve it as following:

use speech synthesis to say the warning message.
send an alert to the user's phone using a library like Twilio.
Please keep in mind that these are just examples, and the specific implementation will depend on the requirements of your application.




itamarizen@gmail.com
Add the use speech synthesis to say the warning message.
send an alert to the user's phone using a library like Twilio as well

To add speech synthesis to say the warning message, you can use the gTTS library which is a Python library and CLI tool to interface with Google Translate's text-to-speech API. You can install it using pip:

Copy code
pip install gTTS
Then, you can use the gTTS library to generate an audio file with the warning message and play it using the pygame library:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Play a warning sound using pygame
        from gtts import gTTS
        tts = gTTS('Warning, obstacles detected')
        tts.save('warning.mp3')
        pygame.init()
        pygame.mixer.music.load("warning.mp3")
        pygame.mixer.music.play()
To send an alert to the user's phone using a library like Twilio, you first need to install the Twilio library using pip:

Copy code
pip install twilio
You also need to sign up for a Twilio account and get an account SID and an auth token.
Then you can use the Twilio library to send an SMS message to the user's phone:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Send an SMS alert using Twilio
        from twilio.rest import Client
        account_sid = 'your_account_sid'
        auth_token = 'your_auth_token'
        client = Client(account_sid, auth_token)
        message = client.messages \
                                .create(
                                     body="Warning, obstacles detected",
                                     from_='your_twilio_phone_number',
                                     to='user_phone_number'
                                 )
It's worth noting that these examples are meant as a starting point and you will need to replace the placeholders in the code with your own Twilio account SID, auth token, and phone numbers.




itamarizen@gmail.com
Any other improvements? My application aims to help wheel chair obstacles avoidance 

Yes, there are many other ways to improve the code for an application that aims to help wheel chair users avoid obstacles. Here are a few suggestions:

Object Detection: Instead of just detecting edges, you can use object detection algorithms like YOLO, Faster R-CNN or RetinaNet to detect specific objects in the image, such as pedestrians, vehicles, or obstacles. This will allow you to not only detect the presence of obstacles, but also determine their type and location.

Obstacle Detection in 3D space: The stereo vision method I suggested earlier can be used to create a 3D point cloud of the environment. This can be used to detect obstacles in 3D space.

Lane Detection: You can use Lane detection algorithms to detect the lane of the wheelchair and make sure that the wheelchair stays within the lane, which can help the user avoid obstacles.

Obstacle Detection in Real-time: The current code captures video frames and processes them one at a time. To make the system more responsive, you can use a real-time obstacle detection method like YOLOv4-tiny or other lightweight deep learning model to detect obstacles in real-time.

Obstacle Avoidance: Once the obstacles have been detected, you can use a path planning algorithm to help the user avoid the obstacles. This can be done using techniques like Dijkstra, A*, or RRT.

Voice assistance: You can use speech recognition and text-to-speech libraries to make the system more accessible for users with visual impairments.

User Feedback: You can add different types of feedback, such as haptic feedback, audio feedback or visual feedback to inform the user about the presence of obstacles and the actions they need to take to avoid them.

Testing: You should test your application in different scenarios to make sure it performs well in different lighting conditions, weather conditions, and with different types of obstacles.

Please keep in mind that these are just a few suggestions, and the specific implementation will depend on the requirements of your application and the resources available to you.




itamarizen@gmail.com
Code for Obstacle Detection in Real-time and User Feedback: You can add different types of feedback, such as haptic feedback, audio feedback or visual feedback to inform the user about the presence of obstacles and the actions they need to take to avoid them.

For obstacle detection in real-time, you can use a real-time object detection algorithm like YOLOv4-tiny or other lightweight deep learning model. You can install the YOLOv4-tiny model using the following command:

Copy code
pip install yolov4
Then, you can use the model to detect objects in the current video frame in real-time:

Copy code
import cv2
import yolov4

# Load the YOLOv4-tiny model
net = yolov4.YOLO()

while True:
    # Capture a frame from each camera
    ret0, frame0 = cap0.read()
    ret1, frame1 = cap1.read()

    # Check if the frames were successfully captured
    if not (ret0 and ret1):
        break

    # Run the YOLOv4-tiny model on the current frame
    objects = net.detect(frame0)

    # Draw bounding boxes around the detected objects
    for obj in objects:
        xmin, ymin, xmax, ymax, class_id, class_name, confidence = obj
        cv2.rectangle(frame0, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
        cv2.putText(frame0, class_name, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the current frame
    cv2.imshow("Real-time Object Detection", frame0)

    # Check if the user pressed 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
Regarding user feedback, one way to add different types of feedback is to use the haptic feedback, audio feedback, or visual feedback.

Haptic feedback can be achieved by using a haptic motor or a vibrating motor, which can be controlled through the Raspberry Pi's GPIO pins. For example, you can use the following code to turn on the haptic motor when an obstacle is detected:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Turn on the haptic motor
        import RPi.GPIO as GPIO
        haptic_pin = 17
        GPIO.setmode(GPIO.BCM)
        GPIO.setup



itamarizen@gmail.com
Continue 

Sure, to continue with the haptic feedback, you can use the following code to turn on the haptic motor when an obstacle is detected:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Turn on the haptic motor
        import RPi.GPIO as GPIO
        haptic_pin = 17
        GPIO.setmode(GPIO.BCM)
        GPIO.setup(haptic_pin, GPIO.OUT)
        GPIO.output(haptic_pin, GPIO.HIGH)
You can also use the following code to turn off the haptic motor after a certain period of time:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Turn on the haptic motor
        import RPi.GPIO as GPIO
        haptic_pin = 17
        GPIO.setmode(GPIO.BCM)
        GPIO.setup(haptic_pin, GPIO.OUT)
        GPIO.output(haptic_pin, GPIO.HIGH)
        time.sleep(0.5)
        GPIO.output(haptic_pin, GPIO.LOW)
For audio feedback, you can use the Pygame library to play a warning sound when an obstacle is detected. As I mentioned earlier in the previous answer, you can use the gTTS library to generate an audio file with the warning message and play it using the pygame library.

For visual feedback, you can use the cv2 library to display a warning message on the screen when an obstacle is detected. For example, you can use the following code to display a warning message on the screen when an obstacle is detected:

Copy code



itamarizen@gmail.com
Continue, code 

Sure, here's an example of how you can display a warning message on the screen when an obstacle is detected:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Display a warning message on the screen
        cv2.putText(frame0, "Warning, obstacles detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
This code will display the text "Warning, obstacles detected" on the frame0 at the position (10, 30) and using the color red. You can adjust the text, font, color, and position to suit your needs.

It's worth noting that you should also test these feedbacks together with the obstacle detection to make sure they are working correctly and the user can understand the feedback clearly.




itamarizen@gmail.com
Code for  visual feedback, you can use the cv2 library to display a warning message on the screen when an obstacle is detected. For example, you can use the following code to display a warning message on the screen when an obstacle is detected again 

Sure, here's an example of how you can display a warning message on the screen when an obstacle is detected using cv2 library:

Copy code
    if cv2.countNonZero(closed) > 0:
        # Display a warning message on the screen
        cv2.putText(frame0, "Warning, obstacles detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
This code will display the text "Warning, obstacles detected" on the frame0 at the position (10, 30) and using the color red. You can adjust the text, font, color, and position to suit your needs.

You can also use the cv2.rectangle function to draw a rectangle around the obstacle in the image, this will help the user to quickly locate the obstacle.

Copy code
    if cv2.countNonZero(closed) > 0:
        # Draw a rectangle around the obstacle
        cv2.rectangle(frame0, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)
You can also use the cv2.circle function to draw a circle around the obstacle in the image, this will help the user to quickly locate the obstacle.

Copy code
    if cv2.countNonZero(closed) > 0:
        # Draw a circle around the obstacle
        cv2.circle(frame0, (x_center, y_center), radius, (0, 0, 255), 2)
You can also use the cv2.arrowedLine function to draw an arrow that indicates the direction of the obstacle in the image, this will help the user to quickly locate the obstacle and avoid it.

Copy code
    if cv2.countNonZero(closed) > 0:
        # Draw an arrow that indicates the direction of the obstacle
        cv2.arrowedLine(frame0, (x_start, y_start), (x_end, y_end), (0, 0, 255), 2)
Please keep in mind that the specific implementation

